<!DOCTYPE html>
<html>
  <head>
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport">
  <meta name="description" content="cocoa_chen&#39;s blog">
  <meta name="keyword" content="objective-C, cocoa_chen, Swift">
  
    <link rel="shortcut icon" href="/css/images/logo.png">
  
  <title>
    
      Scrapy爬虫笔记 | cocoa_chen
    
  </title>
  <link href="//cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet">
  <link href="//cdn.bootcss.com/nprogress/0.2.0/nprogress.min.css" rel="stylesheet">
  <link href="//cdn.bootcss.com/highlight.js/9.12.0/styles/tomorrow.min.css" rel="stylesheet">
  
<link rel="stylesheet" href="/css/style.css">
<link rel="stylesheet" href="/css/plugins/gitment.css">

  <script src="//cdn.bootcss.com/jquery/3.2.1/jquery.min.js"></script>
  <script src="//cdn.bootcss.com/geopattern/1.2.3/js/geopattern.min.js"></script>
  <script src="//cdn.bootcss.com/nprogress/0.2.0/nprogress.min.js"></script>
  
<script src="/js/qrious.js"></script>
<script src="/js/gitment.js"></script>

<meta name="generator" content="Hexo 4.2.1"></head>
<div class="wechat-share">
  <img src="/css/images/logo.png" />
</div>

  <body>
    <header class="header fixed-header">
  <div class="header-container">
    <a class="home-link" href="/">
      <div class="logo"></div>
      <span>cocoa_chen</span>
    </a>
    <ul class="right-list">
      
        <li class="list-item">
          
            <a href="/" class="item-link">Home</a>
          
        </li>
      
        <li class="list-item">
          
            <a href="/tags/" class="item-link">Tags</a>
          
        </li>
      
        <li class="list-item">
          
            <a href="/archives/" class="item-link">Archives</a>
          
        </li>
      
        <li class="list-item">
          
            <a href="/project/" class="item-link">Projects</a>
          
        </li>
      
        <li class="list-item">
          
            <a href="/about/" class="item-link">About</a>
          
        </li>
      
    </ul>
    <div class="menu">
      <span class="icon-bar"></span>
      <span class="icon-bar"></span>
      <span class="icon-bar"></span>
    </div>
    <div class="menu-mask">
      <ul class="menu-list">
        
          <li class="menu-item">
            
              <a href="/" class="menu-link">Home</a>
            
          </li>
        
          <li class="menu-item">
            
              <a href="/tags/" class="menu-link">Tags</a>
            
          </li>
        
          <li class="menu-item">
            
              <a href="/archives/" class="menu-link">Archives</a>
            
          </li>
        
          <li class="menu-item">
            
              <a href="/project/" class="menu-link">Projects</a>
            
          </li>
        
          <li class="menu-item">
            
              <a href="/about/" class="menu-link">About</a>
            
          </li>
        
      </ul>
    </div>
  </div>
</header>

    <div id="article-banner">
  <h2>Scrapy爬虫笔记</h2>
  <p class="post-date">2015-11-17</p>
  <div class="arrow-down">
    <a href="javascript:;"></a>
  </div>
</div>
<main class="app-body flex-box">
  <!-- Article START -->
  <article class="post-article">
    <section class="markdown-content"><p>Scrapy是一个优秀的Python爬虫框架，可以很方便的爬取web站点的信息供我们分析和挖掘，在这记录下最近使用的一些心得。</p>
<a id="more"></a>

<h3 id="1-安装"><a href="#1-安装" class="headerlink" title="1.安装"></a>1.安装</h3><p>通过pip或者easy_install安装:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo pip install scrapy</span><br></pre></td></tr></table></figure>

<h3 id="2-创建爬虫项目"><a href="#2-创建爬虫项目" class="headerlink" title="2.创建爬虫项目"></a>2.创建爬虫项目</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy startproject youProjectName</span><br></pre></td></tr></table></figure>

<h3 id="3-抓取数据"><a href="#3-抓取数据" class="headerlink" title="3.抓取数据"></a>3.抓取数据</h3><p>首先在items.py里定义要抓取的内容,以豆瓣美女为例：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">from scrapy.item import Field,Item</span><br><span class="line"></span><br><span class="line">class DoubanmeinvItem(Item):</span><br><span class="line">    feedId &#x3D; Field()         #feedId</span><br><span class="line">    userId &#x3D; Field()         #用户id</span><br><span class="line">    createOn &#x3D; Field()       #创建时间</span><br><span class="line">    title &#x3D; Field()          #feedTitle</span><br><span class="line">    thumbUrl &#x3D; Field()       #feed缩略图url</span><br><span class="line">    href &#x3D; Field()           #feed链接</span><br><span class="line">    description &#x3D; Field()    #feed简介</span><br><span class="line">    pics &#x3D; Field()           #feed的图片列表</span><br><span class="line">    userInfo &#x3D; Field()       #用户信息</span><br><span class="line"></span><br><span class="line">class UserItem(Item):</span><br><span class="line">    userId &#x3D; Field()         #用户id</span><br><span class="line">    name &#x3D; Field()           #用户name</span><br><span class="line">    avatar &#x3D; Field()         #用户头像</span><br></pre></td></tr></table></figure>

<p>创建爬虫文件,cd到工程文件夹下后输入命令:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy genspider dbMeinv dbmeinv.com</span><br></pre></td></tr></table></figure>

<p>接着编辑爬虫文件，实例如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br></pre></td><td class="code"><pre><span class="line"># -*- coding: utf-8 -*-</span><br><span class="line">import scrapy</span><br><span class="line">import re</span><br><span class="line">from DoubanMeinv.items import DoubanmeinvItem,UserItem</span><br><span class="line">import json</span><br><span class="line">import time</span><br><span class="line">from datetime import datetime</span><br><span class="line">from scrapy.exceptions import CloseSpider</span><br><span class="line"></span><br><span class="line">import sys</span><br><span class="line">reload(sys)</span><br><span class="line">sys.setdefaultencoding(&#39;utf8&#39;)</span><br><span class="line"></span><br><span class="line">class DbmeinvSpider(scrapy.Spider):</span><br><span class="line">    name &#x3D; &quot;dbMeinv&quot;</span><br><span class="line">    allowed_domains &#x3D; [&quot;www.dbmeinv.com&quot;]</span><br><span class="line">    start_urls &#x3D; (</span><br><span class="line">        &#39;http:&#x2F;&#x2F;www.dbmeinv.com&#x2F;dbgroup&#x2F;rank.htm?pager_offset&#x3D;1&#39;,</span><br><span class="line">    )</span><br><span class="line">    baseUrl &#x3D; &#39;http:&#x2F;&#x2F;www.dbmeinv.com&#39;</span><br><span class="line">    close_down &#x3D; False</span><br><span class="line"></span><br><span class="line">    def parse(self, response):</span><br><span class="line">        request &#x3D; scrapy.Request(response.url,callback&#x3D;self.parsePageContent)</span><br><span class="line">        yield request</span><br><span class="line"></span><br><span class="line">    #解析每一页的列表</span><br><span class="line">    def parsePageContent(self, response):</span><br><span class="line">        for sel in response.xpath(&#39;&#x2F;&#x2F;div[@id&#x3D;&quot;main&quot;]&#x2F;&#x2F;li[@class&#x3D;&quot;span3&quot;]&#39;):</span><br><span class="line">            item &#x3D; DoubanmeinvItem()</span><br><span class="line">            title &#x3D; sel.xpath(&#39;.&#x2F;&#x2F;div[@class&#x3D;&quot;bottombar&quot;]&#x2F;&#x2F;a[1]&#x2F;text()&#39;).extract()[0]</span><br><span class="line">            #用strip()方法过滤开头的\r\n\t和空格符</span><br><span class="line">            item[&#39;title&#39;] &#x3D; title.strip()</span><br><span class="line">            item[&#39;thumbUrl&#39;] &#x3D; sel.xpath(&#39;.&#x2F;&#x2F;div[@class&#x3D;&quot;img_single&quot;]&#x2F;&#x2F;img&#x2F;@src&#39;).extract()[0]</span><br><span class="line">            href &#x3D; sel.xpath(&#39;.&#x2F;&#x2F;div[@class&#x3D;&quot;img_single&quot;]&#x2F;a&#x2F;@href&#39;).extract()[0]</span><br><span class="line">            item[&#39;href&#39;] &#x3D; href</span><br><span class="line">            #正则解析id</span><br><span class="line">            pattern &#x3D; re.compile(&quot;dbgroup&#x2F;(\d*)&quot;)</span><br><span class="line">            res &#x3D; pattern.search(href).groups()</span><br><span class="line">            item[&#39;feedId&#39;] &#x3D; res[0]</span><br><span class="line">            #跳转到详情页面</span><br><span class="line">            request &#x3D; scrapy.Request(href,callback&#x3D;self.parseMeinvDetailInfo)</span><br><span class="line">            request.meta[&#39;item&#39;] &#x3D; item</span><br><span class="line">            yield request</span><br><span class="line">        #判断是否超过限制应该停止</span><br><span class="line">        if(self.close_down &#x3D;&#x3D; True):</span><br><span class="line">            print &quot;数据重复，close spider&quot;</span><br><span class="line">            raise CloseSpider(reason &#x3D; &quot;reach max limit&quot;)</span><br><span class="line">        else:</span><br><span class="line">            #获取下一页并加载</span><br><span class="line">            next_link &#x3D; response.xpath(&#39;&#x2F;&#x2F;div[@class&#x3D;&quot;clearfix&quot;]&#x2F;&#x2F;li[@class&#x3D;&quot;next next_page&quot;]&#x2F;a&#x2F;@href&#39;)</span><br><span class="line">            if(next_link):</span><br><span class="line">                url &#x3D; next_link.extract()[0]</span><br><span class="line">                link &#x3D; self.baseUrl + url</span><br><span class="line">                yield scrapy.Request(link,callback&#x3D;self.parsePageContent)</span><br><span class="line"></span><br><span class="line">    #解析详情页面</span><br><span class="line">    def parseMeinvDetailInfo(self, response):</span><br><span class="line">        item &#x3D; response.meta[&#39;item&#39;]</span><br><span class="line">        description &#x3D; response.xpath(&#39;&#x2F;&#x2F;div[@class&#x3D;&quot;panel-body markdown&quot;]&#x2F;p[1]&#x2F;text()&#39;)</span><br><span class="line">        if(description):</span><br><span class="line">            item[&#39;description&#39;] &#x3D; description.extract()[0]</span><br><span class="line">        else:</span><br><span class="line">            item[&#39;description&#39;] &#x3D; &#39;&#39;</span><br><span class="line">        #上传时间</span><br><span class="line">        createOn &#x3D; response.xpath(&#39;&#x2F;&#x2F;div[@class&#x3D;&quot;info&quot;]&#x2F;abbr&#x2F;@title&#39;).extract()[0]</span><br><span class="line">        format &#x3D; &quot;%Y-%m-%d %H:%M:%S.%f&quot;</span><br><span class="line">        t &#x3D; datetime.strptime(createOn,format)</span><br><span class="line">        timestamp &#x3D; int(time.mktime(t.timetuple()))</span><br><span class="line">        item[&#39;createOn&#39;] &#x3D; timestamp</span><br><span class="line">        #用户信息</span><br><span class="line">        user &#x3D; UserItem()</span><br><span class="line">        avatar &#x3D; response.xpath(&#39;&#x2F;&#x2F;div[@class&#x3D;&quot;user-card&quot;]&#x2F;div[@class&#x3D;&quot;pic&quot;]&#x2F;img&#x2F;@src&#39;).extract()[0]</span><br><span class="line">        name &#x3D; response.xpath(&#39;&#x2F;&#x2F;div[@class&#x3D;&quot;user-card&quot;]&#x2F;div[@class&#x3D;&quot;info&quot;]&#x2F;&#x2F;li[@class&#x3D;&quot;name&quot;]&#x2F;text()&#39;).extract()[0]</span><br><span class="line">        home &#x3D; response.xpath(&#39;&#x2F;&#x2F;div[@class&#x3D;&quot;user-card&quot;]&#x2F;div[@class&#x3D;&quot;opt&quot;]&#x2F;a[@target&#x3D;&quot;_users&quot;]&#x2F;@href&#39;).extract()[0]</span><br><span class="line">        user[&#39;avatar&#39;] &#x3D; avatar</span><br><span class="line">        user[&#39;name&#39;] &#x3D; name</span><br><span class="line">        #正则解析id</span><br><span class="line">        pattern &#x3D; re.compile(&quot;&#x2F;users&#x2F;(\d*)&quot;)</span><br><span class="line">        res &#x3D; pattern.search(home).groups()</span><br><span class="line">        user[&#39;userId&#39;] &#x3D; res[0]</span><br><span class="line">        item[&#39;userId&#39;] &#x3D; res[0]</span><br><span class="line">        #将item关联user</span><br><span class="line">        item[&#39;userInfo&#39;] &#x3D; user</span><br><span class="line">        #解析链接</span><br><span class="line">        pics &#x3D; []</span><br><span class="line">        links &#x3D; response.xpath(&#39;&#x2F;&#x2F;div[@class&#x3D;&quot;panel-body markdown&quot;]&#x2F;div[@class&#x3D;&quot;topic-figure cc&quot;]&#39;)</span><br><span class="line">        if(links):</span><br><span class="line">            for a in links:</span><br><span class="line">                img &#x3D; a.xpath(&#39;.&#x2F;img&#x2F;@src&#39;)</span><br><span class="line">                if(img):</span><br><span class="line">                    picUrl &#x3D; img.extract()[0]</span><br><span class="line">                    pics.append(picUrl)</span><br><span class="line">        #转成json字符串保存</span><br><span class="line">        item[&#39;pics&#39;] &#x3D; json.dumps(list(pics))</span><br><span class="line">        yield item</span><br></pre></td></tr></table></figure>

<p>需要说明的几点内容：</p>
<ul>
<li><code>allowed_domin</code>指定Spider在哪个网站爬取数据</li>
<li><code>start_urls</code>包含了Spider在启动时进行爬取的url列表</li>
<li><code>parse方法</code>继承自父类，每个初始URL完成下载后生成的Response对象将会作为唯一的参数传递给该函数。该方法负责解析返回的数据(response)，提取数据(生成item)以及生成需要进一步处理的URL的Request对象</li>
<li><code>xpath</code>解析数据的时候使用(也可以使用css)，关于xpath和css的详细用法请自行搜索</li>
<li><code>xpath</code>从某个子元素里解析数据时要使用<code>element.xpath(&#39;./***&#39;)</code>而不能使用<code>element.xpath(&#39;/***&#39;)</code>，否则是从最外层解析而不是从element下开始解析</li>
<li>web站点爬取的text经常包含了我们不想要的\r\n\t或者是空格等字符,这个时候就要使用Python的<code>strip()</code>方法来过滤掉这些数据</li>
<li>抓取的web页面时间经常是2015-10-1 12:00:00格式，但是我们存储到数据库时要想转成timeStamp的格式，这里用Python的time相关类库来处理,代码见上面</li>
<li>抓取完某个页面的时候，可能我们还需要抓取跟它相关的详情页面数据，这里用生成<code>Scrapy.Request</code>的方式来继续抓取,并且将当前的item存储到新的request的meta数据中以供后面的代码中读取到已抓取的item</li>
<li>如果我们想要在某些情况下停止Spider的抓取，在这里设置一个flag位,并在适当的地方抛出一个<code>CloseSpider</code>的异常来停止爬虫，后面会接着提到这个技巧</li>
</ul>
<h3 id="4-运行爬虫"><a href="#4-运行爬虫" class="headerlink" title="4.运行爬虫"></a>4.运行爬虫</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy crawl youSpiderName</span><br></pre></td></tr></table></figure>

<h3 id="5-编写Pipeline"><a href="#5-编写Pipeline" class="headerlink" title="5.编写Pipeline"></a>5.编写Pipeline</h3><p>如果我们要将数据存储到MySQL数据库中，需要安装MySQLdb，安装过程很多坑，遇到了再Google解决吧。一切搞定之后开始编写pipelines.py和settings.py文件</p>
<p>首先在settings.py文件中定义好连接MySQL数据库的所需信息，如下所示:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">DB_SERVER &#x3D; &#39;MySQLdb&#39;</span><br><span class="line">DB_CONNECT &#x3D; &#123;</span><br><span class="line">    &#39;host&#39; : &#39;localhost&#39;,</span><br><span class="line">    &#39;user&#39; : &#39;root&#39;,</span><br><span class="line">    &#39;passwd&#39; : &#39;&#39;,</span><br><span class="line">    &#39;port&#39; : 3306,</span><br><span class="line">    &#39;db&#39; :&#39;dbMeizi&#39;,</span><br><span class="line">    &#39;charset&#39; : &#39;utf8&#39;,</span><br><span class="line">    &#39;use_unicode&#39; : True</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>然后编辑pipelines.py文件，添加代码如下:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br></pre></td><td class="code"><pre><span class="line">from scrapy.conf import settings</span><br><span class="line">from scrapy.exceptions import DropItem</span><br><span class="line">from twisted.enterprise import adbapi</span><br><span class="line">import json</span><br><span class="line"></span><br><span class="line">class DoubanmeinvPipeline(object):</span><br><span class="line">    #插入的sql语句</span><br><span class="line">    feed_key &#x3D; [&#39;feedId&#39;,&#39;userId&#39;,&#39;createOn&#39;,&#39;title&#39;,&#39;thumbUrl&#39;,&#39;href&#39;,&#39;description&#39;,&#39;pics&#39;]</span><br><span class="line">    user_key &#x3D; [&#39;userId&#39;,&#39;name&#39;,&#39;avatar&#39;]</span><br><span class="line">    insertFeed_sql &#x3D; &#39;&#39;&#39;insert into MeiziFeed (%s) values (%s)&#39;&#39;&#39;</span><br><span class="line">    insertUser_sql &#x3D; &#39;&#39;&#39;insert into MeiziUser (%s) values (%s)&#39;&#39;&#39;</span><br><span class="line">    feed_query_sql &#x3D; &quot;select * from MeiziFeed where feedId &#x3D; %s&quot;</span><br><span class="line">    user_query_sql &#x3D; &quot;select * from MeiziUser where userId &#x3D; %s&quot;</span><br><span class="line">    feed_seen_sql &#x3D; &quot;select feedId from MeiziFeed&quot;</span><br><span class="line">    user_seen_sql &#x3D; &quot;select userId from MeiziUser&quot;</span><br><span class="line">    max_dropcount &#x3D; 50</span><br><span class="line">    current_dropcount &#x3D; 0</span><br><span class="line"></span><br><span class="line">    def __init__(self):</span><br><span class="line">        dbargs &#x3D; settings.get(&#39;DB_CONNECT&#39;)</span><br><span class="line">        db_server &#x3D; settings.get(&#39;DB_SERVER&#39;)</span><br><span class="line">        dbpool &#x3D; adbapi.ConnectionPool(db_server,**dbargs)</span><br><span class="line">        self.dbpool &#x3D; dbpool</span><br><span class="line">        #更新看过的id列表</span><br><span class="line">        d &#x3D; self.dbpool.runInteraction(self.update_feed_seen_ids)</span><br><span class="line">        d.addErrback(self._database_error)</span><br><span class="line">        u &#x3D; self.dbpool.runInteraction(self.update_user_seen_ids)</span><br><span class="line">        u.addErrback(self._database_error)</span><br><span class="line"></span><br><span class="line">    def __del__(self):</span><br><span class="line">        self.dbpool.close()</span><br><span class="line"></span><br><span class="line">    #更新feed已录入的id列表</span><br><span class="line">    def update_feed_seen_ids(self, tx):</span><br><span class="line">        tx.execute(self.feed_seen_sql)</span><br><span class="line">        result &#x3D; tx.fetchall()</span><br><span class="line">        if result:</span><br><span class="line">            #id[0]是因为result的子项是tuple类型</span><br><span class="line">            self.feed_ids_seen &#x3D; set([int(id[0]) for id in result])</span><br><span class="line">        else:</span><br><span class="line">            #设置已查看过的id列表</span><br><span class="line">            self.feed_ids_seen &#x3D; set()</span><br><span class="line"></span><br><span class="line">    #更新user已录入的id列表</span><br><span class="line">    def update_user_seen_ids(self, tx):</span><br><span class="line">        tx.execute(self.user_seen_sql)</span><br><span class="line">        result &#x3D; tx.fetchall()</span><br><span class="line">        if result:</span><br><span class="line">            #id[0]是因为result的子项是tuple类型</span><br><span class="line">            self.user_ids_seen &#x3D; set([int(id[0]) for id in result])</span><br><span class="line">        else:</span><br><span class="line">            #设置已查看过的id列表</span><br><span class="line">            self.user_ids_seen &#x3D; set()</span><br><span class="line"></span><br><span class="line">    #处理每个item并返回</span><br><span class="line">    def process_item(self, item, spider):</span><br><span class="line">        query &#x3D; self.dbpool.runInteraction(self._conditional_insert, item)</span><br><span class="line">        query.addErrback(self._database_error, item)</span><br><span class="line"></span><br><span class="line">        feedId &#x3D; item[&#39;feedId&#39;]</span><br><span class="line">        if(int(feedId) in self.feed_ids_seen):</span><br><span class="line">            self.current_dropcount +&#x3D; 1</span><br><span class="line">            if(self.current_dropcount &gt;&#x3D; self.max_dropcount):</span><br><span class="line">                spider.close_down &#x3D; True</span><br><span class="line">            raise DropItem(&quot;重复的数据:%s&quot; % item[&#39;feedId&#39;])</span><br><span class="line">        else:</span><br><span class="line">            return item</span><br><span class="line"></span><br><span class="line">    #插入数据</span><br><span class="line">    def _conditional_insert(self, tx, item):</span><br><span class="line">        #插入Feed</span><br><span class="line">        tx.execute(self.feed_query_sql, (item[&#39;feedId&#39;]))</span><br><span class="line">        result &#x3D; tx.fetchone()</span><br><span class="line">        if result &#x3D;&#x3D; None:</span><br><span class="line">            self.insert_data(item,self.insertFeed_sql,self.feed_key)</span><br><span class="line">        else:</span><br><span class="line">            print &quot;该feed已存在数据库中:%s&quot; % item[&#39;feedId&#39;]</span><br><span class="line">        #添加进seen列表中</span><br><span class="line">        feedId &#x3D; item[&#39;feedId&#39;]</span><br><span class="line">        if int(feedId) not in self.feed_ids_seen:</span><br><span class="line">            self.feed_ids_seen.add(int(feedId))</span><br><span class="line">        #插入User</span><br><span class="line">        user &#x3D; item[&#39;userInfo&#39;]</span><br><span class="line">        tx.execute(self.user_query_sql, (user[&#39;userId&#39;]))</span><br><span class="line">        user_result &#x3D; tx.fetchone()</span><br><span class="line">        if user_result &#x3D;&#x3D; None:</span><br><span class="line">            self.insert_data(user,self.insertUser_sql,self.user_key)</span><br><span class="line">        else:</span><br><span class="line">            print &quot;该用户已存在数据库:%s&quot; % user[&#39;userId&#39;]</span><br><span class="line">        #添加进seen列表中</span><br><span class="line">        userId &#x3D; user[&#39;userId&#39;]</span><br><span class="line">        if int(userId) not in self.user_ids_seen:</span><br><span class="line">            self.user_ids_seen.add(int(userId))</span><br><span class="line"></span><br><span class="line">    #插入数据到数据库中</span><br><span class="line">    def insert_data(self, item, insert, sql_key):</span><br><span class="line">        fields &#x3D; u&#39;,&#39;.join(sql_key)</span><br><span class="line">        qm &#x3D; u&#39;,&#39;.join([u&#39;%s&#39;] * len(sql_key))</span><br><span class="line">        sql &#x3D; insert % (fields,qm)</span><br><span class="line">        data &#x3D; [item[k] for k in sql_key]</span><br><span class="line">        return self.dbpool.runOperation(sql,data)</span><br><span class="line"></span><br><span class="line">    #数据库错误</span><br><span class="line">    def _database_error(self, e):</span><br><span class="line">        print &quot;Database error: &quot;, e</span><br></pre></td></tr></table></figure>

<p>说明几点内容：</p>
<ul>
<li><code>process_item</code>:每个item通过pipeline组件都需要调用该方法，这个方法必须返回一个Item对象,或者抛出DropItem异常，被丢弃的item将不会被之后的pipeline组件所处理。</li>
<li>已经抓取到的数据不应该再处理，这里创建了两个ids_seen方法来保存已抓取的id数据，如果已存在就Drop掉item</li>
<li>如果重复抓取的数据过多时，这里设置了个上限值(50),如果超过了上限值就改变spider的关闭flag标志位，然后spider判断flag值在适当的时候抛出<code>CloseSpider</code>异常，关闭Spider代码见爬虫文件。这里通过设置flag标志位的方式来关闭爬虫主要是因为我测试的时候发现在pipelines中调用停止爬虫的方法都不起效果，故改成这种方式</li>
<li>因为Scrapy是基于twisted的，所以这里用adbapi来连接并操作MySQL数据库</li>
</ul>
<p>最后在settings.py文件中启用pipeline</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ITEM_PIPELINES &#x3D; &#123;</span><br><span class="line">   &#39;DoubanMeinv.pipelines.DoubanmeinvPipeline&#39;: 300,</span><br><span class="line">   # &#39;DoubanMeinv.pipelines.ImageCachePipeline&#39;: 500,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="6-变换User-Agent，避免爬虫被ban"><a href="#6-变换User-Agent，避免爬虫被ban" class="headerlink" title="6.变换User-Agent，避免爬虫被ban"></a>6.变换User-Agent，避免爬虫被ban</h3><p>我们抓取的网站可能会检查User-Agent，所以为了爬虫正常运行我们需要设置请求的User-Agent。对于频繁的请求，还要对User-Agent做随机变换以防被ban,这里通过设置Downloader Middleware来修改爬虫的request和respons</p>
<p>在setting.py文件中添加User-Agent列表</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">DOWNLOADER_MIDDLEWARES &#x3D; &#123;</span><br><span class="line">   &#39;DoubanMeinv.middlewares.RandomUserAgent&#39;: 1,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">USER_AGENTS &#x3D; [</span><br><span class="line">    &quot;Mozilla&#x2F;4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; AcooBrowser; .NET CLR 1.1.4322; .NET CLR 2.0.50727)&quot;,</span><br><span class="line">    &quot;Mozilla&#x2F;4.0 (compatible; MSIE 7.0; AOL 9.5; AOLBuild 4337.35; Windows NT 5.1; .NET CLR 1.1.4322; .NET CLR 2.0.50727)&quot;,</span><br><span class="line">    &quot;Mozilla&#x2F;5.0 (X11; U; Linux; en-US) AppleWebKit&#x2F;527+ (KHTML, like Gecko, Safari&#x2F;419.3) Arora&#x2F;0.6&quot;,</span><br><span class="line">    &quot;Mozilla&#x2F;5.0 (Windows NT 6.1; WOW64) AppleWebKit&#x2F;535.11 (KHTML, like Gecko) Chrome&#x2F;17.0.963.56 Safari&#x2F;535.11&quot;,</span><br><span class="line">    &quot;Mozilla&#x2F;5.0 (Macintosh; Intel Mac OS X 10_7_3) AppleWebKit&#x2F;535.20 (KHTML, like Gecko) Chrome&#x2F;19.0.1036.7 Safari&#x2F;535.20&quot;,</span><br><span class="line">]</span><br></pre></td></tr></table></figure>

<p>修改middlewares.py文件添加如下代码：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">import random</span><br><span class="line"></span><br><span class="line">class RandomUserAgent(object):</span><br><span class="line">    def __init__(self, agents):</span><br><span class="line">        self.agents &#x3D; agents</span><br><span class="line"></span><br><span class="line">    @classmethod</span><br><span class="line">    def from_crawler(cls, crawler):</span><br><span class="line">        return cls(crawler.settings.getlist(&#39;USER_AGENTS&#39;))</span><br><span class="line"></span><br><span class="line">    def process_request(self, request, spider):</span><br><span class="line">        request.headers.setdefault(&#39;User-Agent&#39;, random.choice(self.agents))</span><br></pre></td></tr></table></figure>

<h3 id="7-禁用Cookie-设置请求延迟"><a href="#7-禁用Cookie-设置请求延迟" class="headerlink" title="7.禁用Cookie+设置请求延迟"></a>7.禁用Cookie+设置请求延迟</h3><p>某些网站可能会根据cookie来分析爬取的轨迹，为了被ban，我们最好也禁用掉cookie;同时为了避免请求太频繁而造成爬虫被ban，我们还需要设置请求间隔时间,在settings.py文件中添加以下代码:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">DOWNLOAD_DELAY&#x3D;1</span><br><span class="line">COOKIES_ENABLED&#x3D;False</span><br></pre></td></tr></table></figure>

<h3 id="8-抓取图片并保存到本地"><a href="#8-抓取图片并保存到本地" class="headerlink" title="8.抓取图片并保存到本地"></a>8.抓取图片并保存到本地</h3><p>有时候我们想把抓取到的图片直接下载并保存到本地，可以用Scrapy内置的<code>ImagesPipeline</code>来处理,因为<code>ImagesPipeline</code>用到了PIL这个图片处理模块，所以我们首先需要使用pip来安装<code>Pillow</code></p>
<p>安装成功后，在pipelines.py代码中添加以下代码：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">from scrapy.pipelines.images import ImagesPipeline</span><br><span class="line">from scrapy import Request</span><br><span class="line">import json</span><br><span class="line"></span><br><span class="line">class ImageCachePipeline(ImagesPipeline):</span><br><span class="line">    def get_media_requests(self, item, info):</span><br><span class="line">        pics &#x3D; item[&#39;pics&#39;]</span><br><span class="line">        list &#x3D; json.loads(pics)</span><br><span class="line">        for image_url in list:</span><br><span class="line">            yield Request(image_url)</span><br><span class="line"></span><br><span class="line">    def item_completed(self, results, item, info):</span><br><span class="line">        image_paths&#x3D;[x[&#39;path&#39;] for ok,x in results if ok]</span><br><span class="line">        if not image_paths:</span><br><span class="line">            print &quot;图片未下载好:%s&quot; % image_paths</span><br><span class="line">            raise DropItem(&#39;图片未下载好 %s&#39;%image_paths)</span><br></pre></td></tr></table></figure>

<p>ImagesPipeline类有一个<code>get_media_requests</code>方法来进行下载的控制，所以我们在这里解析imgUrl并发起进行一个Request,在下载完成之后，会把结果传递到<code>item_completed</code>方法，包括 下载是否成功（ True or False) 以及下载下来保存的路径和下载的路径，这里改写这个方法让他把下载失败的（Flase）的图片的路径输出出来</p>
<p>接下来在settings.py里设置下载图片的文件目录并启用ImageCachePipeline</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">#设置图片保存到本地的地址和过期时间</span><br><span class="line">IMAGES_STORE&#x3D;&#39;&#x2F;Users&#x2F;chen&#x2F;Pictures&#x2F;Meizi&#39;</span><br><span class="line">IMAGES_EXPIRES &#x3D; 90</span><br><span class="line"></span><br><span class="line">ITEM_PIPELINES &#x3D; &#123;</span><br><span class="line">   &#39;DoubanMeinv.pipelines.DoubanmeinvPipeline&#39;: 300,</span><br><span class="line">   &#39;DoubanMeinv.pipelines.ImageCachePipeline&#39;: 500,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>等待爬虫执行完之后去IMAGES_STORE路径下查看图片就是了</p>
<h3 id="9-自动运行爬虫"><a href="#9-自动运行爬虫" class="headerlink" title="9.自动运行爬虫"></a>9.自动运行爬虫</h3><p>为了源源不断获取数据，可通过命令让爬虫每天都运行来抓取数据</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; 为当前用户新增任务</span><br><span class="line">crontab -e</span><br><span class="line">&#x2F;&#x2F; 增加如下记录 注意替换自己的爬虫目录 由于环境变量的原因，scrapy要给出全路径</span><br><span class="line">0 10 * * * cd &#x2F;home&#x2F;chen&#x2F;pyWork&#x2F;DoubanMeinvScrapy &amp;&amp; &#x2F;usr&#x2F;bin&#x2F;scrapy crawl dbmeinv</span><br></pre></td></tr></table></figure>
<p>上面的命令添加了一个任务，这个任务会每天早上10：00启动，这个任务要做得就是进入爬虫目录，并启动爬虫。<br>如果你不知道自己的scrapy的全路径，可以用终端下用<code>which scrapy</code>来查看<br><br/></p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结:"></a>总结:</h3><p>以上的内容只是自己在使用Scrapy过程中的一些心得体会，算是简单入门Scrapy了，接下来还得了解分布式爬取以及一些更高级的技巧，以后有新的经验了再更新。Enjoy it!</p>
</section>
    <!-- Tags START -->
    
      <div class="tags">
        <span>Tags:</span>
        
  <a href="/tags#iOS" >
    <span class="tag-code">iOS</span>
  </a>

      </div>
    
    <!-- Tags END -->
    <!-- 打赏 START -->
    
      <div class="money-like">
        <div class="reward-btn">
          赏
          <span class="money-code">
            <span class="alipay-code">
              <div class="code-image"></div>
              <b>使用支付宝打赏</b>
            </span>
            <span class="wechat-code">
              <div class="code-image"></div>
              <b>使用微信打赏</b>
            </span>
          </span>
        </div>
        <p class="notice">若你觉得我的文章对你有帮助，欢迎点击上方按钮对我打赏</p>
      </div>
    
    <!-- 打赏 END -->
    <!-- 二维码 START -->
    <div class="qrcode">
      <canvas id="share-qrcode"></canvas>
      <p class="notice">扫描二维码，分享此文章</p>
    </div>
    <!-- 二维码 END -->
    
      <!-- Gitment START -->
      <div id="comments"></div>
      <!-- Gitment END -->
    
  </article>
  <!-- Article END -->
  <!-- Catalog START -->
  
  <!-- Catalog END -->
</main>

<script>
  (function () {
    var url = 'http://yoursite.com/2015/11/17/scrapypa-chong-bi-ji/';
    var banner = ''
    if (banner !== '' && banner !== 'undefined' && banner !== 'null') {
      $('#article-banner').css({
        'background-image': 'url(' + banner + ')'
      })
    } else {
      $('#article-banner').geopattern(url)
    }
    $('.header').removeClass('fixed-header')

     // error image
    $(".markdown-content img").on('error', function() {
      $(this).attr('src', 'http://file.muyutech.com/error-img.png')
      $(this).css({
        'cursor': 'default'
      })
    })

    // zoom image
    $(".markdown-content img").on('click', function() {
      var src = $(this).attr('src')
      if (src !== 'http://file.muyutech.com/error-img.png') {
        var imageW = $(this).width()
        var imageH = $(this).height()
        
        var zoom = ($(window).width() * 0.95 / imageW).toFixed(2)
        zoom = zoom < 1 ? 1 : zoom
        zoom = zoom > 2 ? 2 : zoom
        var transY = (($(window).height() - imageH) / 2).toFixed(2)

        $('body').append('<div class="image-view-wrap"><div class="image-view-inner"><img src="'+ src +'" /></div></div>')
        $('.image-view-wrap').addClass('wrap-active')
        $('.image-view-wrap img').css({
          'width': `${imageW}`,
          'transform': `translate3d(0, ${transY}px, 0) scale3d(${zoom}, ${zoom}, 1)`
        })
        $('html').css('overflow', 'hidden')

        $('.image-view-wrap').on('click', function() {
          $(this).remove()
          $('html').attr('style', '')
        })
      }
    })

    // qrcode
    var qr = new QRious({
      element: document.getElementById('share-qrcode'),
      value: document.location.href
    });

    // gitment
    var gitmentConfig = "cocoa-chen";
    if (gitmentConfig !== 'undefined') {
      var gitment = new Gitment({
        id: "Scrapy爬虫笔记",
        owner: "cocoa-chen",
        repo: "cocoa-chen.github.io",
        oauth: {
          client_id: "e12c4d16bb2a811822a7",
          client_secret: "8fa9d6ce27d985bc1ed6027ee666305208a5e86e"
        },
        theme: {
          render(state, instance) {
            const container = document.createElement('div')
            container.lang = "en-US"
            container.className = 'gitment-container gitment-root-container'
            container.appendChild(instance.renderHeader(state, instance))
            container.appendChild(instance.renderEditor(state, instance))
            container.appendChild(instance.renderComments(state, instance))
            container.appendChild(instance.renderFooter(state, instance))
            return container;
          }
        }
      })
      gitment.render(document.getElementById('comments'))
    }
  })();
</script>

    <div class="scroll-top">
  <span class="arrow-icon"></span>
</div>
    <footer class="app-footer">
  <p class="copyright">
    &copy; 2020 | Proudly powered by <a href="https://hexo.io" target="_blank">Hexo</a>
    <br>
  </p>
</footer>

<script>
  function async(u, c) {
    var d = document, t = 'script',
      o = d.createElement(t),
      s = d.getElementsByTagName(t)[0];
    o.src = u;
    if (c) { o.addEventListener('load', function (e) { c(null, e); }, false); }
    s.parentNode.insertBefore(o, s);
  }
</script>
<script>
  async("//cdn.bootcss.com/fastclick/1.0.6/fastclick.min.js", function(){
    FastClick.attach(document.body);
  })
</script>

<script>
  var hasLine = 'true';
  async("//cdn.bootcss.com/highlight.js/9.12.0/highlight.min.js", function(){
    $('figure pre').each(function(i, block) {
      var figure = $(this).parents('figure');
      if (hasLine === 'false') {
        figure.find('.gutter').hide();
      }
      var lang = figure.attr('class').split(' ')[1] || 'code';
      var codeHtml = $(this).html();
      var codeTag = document.createElement('code');
      codeTag.className = lang;
      codeTag.innerHTML = codeHtml;
      $(this).attr('class', '').empty().html(codeTag);
      figure.attr('data-lang', lang.toUpperCase());
      hljs.highlightBlock(block);
    });
  })
</script>
<!-- Baidu Tongji -->


<script src="/js/script.js"></script>

  </body>
</html>